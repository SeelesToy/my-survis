define({ entries : {
    "Braun2019CUDAFlux": {
        "abstract": "GPUs are powerful, massively parallel processors, which require a vast amount of thread parallelism to keep their thousands of execution units busy, and to tolerate latency when accessing its high-throughput memory system. Understanding the behavior of massively threaded GPU programs can be difficult, even though recent GPUs provide an abundance of hardware performance counters, which collect statistics about certain events. Profiling tools that assist the user in such analysis for their GPUs, like NVIDIA's nvprof and cupti, are state-of-the-art. However, instrumentation based on reading hardware performance counters can be slow, in particular when the number of metrics is large. Furthermore, the results can be inaccurate as instructions are grouped to match the available set of hardware counters.",
        "author": "Lorenz Braun and H. Fr\u00f6ning",
        "doi": "10.1109/PMBS49563.2019.00014",
        "journal": "2019 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)",
        "keywords": "GPU, CUDA, LLVM, Profiling, PTX",
        "pages": "73-81",
        "title": "CUDA Flux: A Lightweight Instruction Profiler for CUDA Applications",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9059260",
        "volume": "null",
        "year": "2019"
    },
    "Elhelw2020Modular": {
        "abstract": "Graphics Processing Units (GPUs) are widely used to run general-purpose computing workloads. Three approaches currently exist to observe the dynamic behaviour of these workloads: real hardware, architectural simulators, and functional simulators (or emulators). However, the rapid evolution of GPU hardware and software stacks means that, in reality, using hardware is the only option to study current GPU workloads. Unfortunately, GPU toolchain support for advanced characterization capabilities is still far behind CPU toolchains like Pin. In this paper, we present an early glimpse of Horus, an emulator for NVIDIA GPUs. Although it is not the first such emulator, Horus is being engineered using a novel methodology to keep pace with the rapid changes in GPU hardware. Horus is highly modular, and is the first to utilize a specially designed DSL for specifying formal semantics for GPU instruction sets (NVIDIA PTX). A semantics compiler uses these semantics to generate the emulator. Horus also features a well-defined interface by which utility functions \u2013 instrumentation, new instruction support, analysis tools \u2013 can be coupled with the main emulator to increase reuse while studying the dynamic behaviour of GPU kernels. Horus is now mature enough to run all the Polybench and Rodinia benchmarks correctly.",
        "author": "A. Elhelw and Sreepathi Pai",
        "doi": "10.1109/ISPASS48437.2020.00020",
        "journal": "2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)",
        "keywords": "functional simulator, formal semantics, GPU, instrumentation",
        "pages": "104-106",
        "title": "Horus: A Modular GPU Emulator Framework",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9238604",
        "volume": "null",
        "year": "2020"
    },
    "Fang2017ARMv8Multi-CoreCPU": {
        "abstract": "The OpenCL standard allows targeting a large variety of CPU, GPU and accelerator architectures using a single unified programming interface and language. But guaranteeing portability relies heavily on platform-specific implementations. In this paper, we provide an OpenCL implementation on an ARMv8 multi-core CPU, which efficiently maps the generic OpenCL platform model to the ARMv8 multi-core architecture. With this implementation, we first characterize the maximum achieved arithmetic throughput and memory accessing bandwidth on the architecture, and measure the OpenCL-related overheads. Our results demonstrate that there exists an optimization room for improving OpenCL kernel performance. Then, we compare the performance of OpenCL against serial codes and OpenMP codes with 11 benchmarks. The experimental results show that (1) the OpenCL implementation can achieve an average speedup of 6X compared to its OpenMP counterpart, and (2) the GPU-specified OpenCL codes are often unsuitable for this ARMv8 multi-core CPU.",
        "author": "Jianbin Fang and P. Zhang and T. Tang and Chun Huang and Canqun Yang",
        "doi": "10.1109/ISPA/IUCC.2017.00131",
        "journal": "2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC)",
        "keywords": "OpenCL, FT-1500A, performance, programming",
        "pages": "860-867",
        "title": "Implementing and Evaluating OpenCL on an ARMv8 Multi-Core CPU",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/8367361",
        "volume": "null",
        "year": "2017"
    },
    "Farooqui2014GPGPU": {
        "abstract": "Dynamic instrumentation of GPGPU binaries makes possible real-time introspection methods for performance debugging, correctness checks, workload characterization, and runtime optimization. Such instrumentation involves inserting code at the instruction level of an application, while the application is running, thereby able to accurately profile data-dependent application behavior. Runtime overheads seen from instrumentation, however, can obviate its utility. This paper shows how a combination of information flow analysis and symbolic execution can be used to alleviate these overheads. The methods and their effectiveness are demonstrated for a variety of GPGPU codes written in OpenCL that run on AMD GPU target backends. Kernels that can be analyzed entirely via symbolic execution need not be instrumented, thus eliminating kernel runtime overheads altogether. For the remaining GPU kernels, our results show 5-38% improvements in kernel runtime overheads.",
        "author": "N. Farooqui and K. Schwan and S. Yalamanchili",
        "doi": "10.1145/2576779.2576782",
        "keywords": "OpenCL, CUDA, GPGPU, Rodinia",
        "title": "Efficient Instrumentation of GPGPU Applications Using Information Flow Analysis and Symbolic Execution",
        "type": "article",
        "url": "https://dl.acm.org/doi/10.1145/2588768.2576782",
        "year": "2014"
    },
    "Guo2011Synchronizations": {
        "abstract": "Automatic compilation for multiple types of devices is important, especially given the current trends towards heterogeneous computing. This paper concentrates on some issues in compiling fine-grained SPMD-threaded code (e.g., GPU CUDA code) for multicore CPUs. It points out some correctness pitfalls in existing techniques, particularly in their treatment to implicit synchronizations. It then describes a systematic dependence analysis specially designed for handling implicit synchronizations in SPMD-threaded programs. By unveiling the relations between inter-thread data dependences and correct treatment to synchronizations, it presents a dependence-based solution to the problem. Experiments demonstrate that the proposed techniques can resolve the correctness issues in existing compilation techniques, and help compilers produce correct and efficient translation results.",
        "author": "Ziyu Guo and E. Zhang and Xipeng Shen",
        "doi": "10.1109/PACT.2011.62",
        "journal": "2011 International Conference on Parallel Architectures and Compilation Techniques",
        "keywords": "GPU, CUDA, GPU-to-CPU Translation, Implicit Synchronizations, Dependence Analysis, SPMD-Translation",
        "pages": "310-319",
        "title": "Correctly Treating Synchronizations in Compiling Fine-Grained SPMD-Threaded Programs for CPU",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/6113839",
        "volume": "null",
        "year": "2011"
    },
    "Han2021CUDAonX86": {
        "abstract": "As CUDA programs become the de facto program among data parallel applications such as highperformance computing or machine learning applications, running CUDA on other platforms has been a compelling option. Although several efforts have attempted to support CUDA on other than NVIDIA GPU devices, due to extra steps in the translation, the support is always behind a few years from supporting CUDA\u2019s latest features. The examples are DPC, Hipfy, where CUDA source code have to be translated to their native supporting language and then they are supported. In particular, the new CUDA programming model exposes the warp concept in the programming language, which greatly changes the way the CUDA code should be mapped to CPU programs. In this paper, hierarchical collapsing that correctly supports CUDA warp-level functions on CPUs is proposed. Based on hierarchical collapsing, a framework, COX, is developed that allows CUDA programs with the latest features to be executed efficiently on CPU platforms. COX consists of a compiler IR transformation (new LLVM pass) and a runtime system to execute the transformed programs on CPU devices. COX can support the most recent CUDA features, and the application coverage is much higher (90%) than for previous frameworks (68%) with comparable performance. We also show that the warp-level functions in CUDA can be efficiently executed by utilizing CPU SIMD (AVX) instructions.",
        "arxivid": "2112.10034",
        "author": "Ruobing Han and Jaewon Lee and Jaewoong Sim and Hyesoon Kim",
        "journal": "ArXiv",
        "keywords": "GPU, code migration, compiler transformations",
        "pages": "null",
        "title": "COX: CUDA on X86 by Exposing Warp-Level Functions to CPUs",
        "type": "article",
        "url": "https://www.semanticscholar.org/paper/ffb3625b9b409cf7747b7a24a27808a2664f82a7",
        "volume": "abs/2112.10034",
        "year": "2021"
    },
    "Jin2021EvaluatingCUDA": {
        "abstract": "HIPCL is expanding the scope of the CUDA portability route from an AMD platform to an OpenCL platform. In the meantime, the Intel DPC++ Compatibility Tool (DPCT) is migrating a CUDA program to a data parallel C++ (DPC++) program. Towards the goal of portability enhancement, we evaluate the performance of the CUDA applications from Rodinia, SHOC, and proxy applications ported using HIPCL and DPCT on Intel GPUs. After profiling the ported programs, we aim to understand their performance gaps, and optimize codes converted by DPCT to improve their performance. The open-source repository for the CUDA, HIP, and DPCT programs will be useful for the development of a translator.",
        "author": "Zheming Jin and J. Vetter",
        "doi": "10.1109/IPDPSW52791.2021.00065",
        "journal": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)",
        "keywords": "CUDA, HIP, OpenCL, DPC++, CUDA Portability",
        "pages": "371-376",
        "title": "Evaluating CUDA Portability with HIPCL and DPCT",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9460636",
        "volume": "null",
        "year": "2021"
    },
    "Kaszyk2019Full-System": {
        "abstract": "Graphics Processing Units (GPUs) critically rely on a complex system software stack comprising kernel- and user-space drivers and Just-in-time (JIT) compilers. Yet, existing GPU simulators typically abstract away details of the software stack and GPU instruction set. Partly, this is because GPU vendors rarely release sufficient information about their latest GPU products. However, this is also due to the lack of an integrated CPU/GPU simulation framework, which is complete and powerful enough to drive the complex GPU software environment. This has led to a situation where research on GPU architectures and compilers is largely based on outdated or greatly simplified architectures and software stacks, undermining the validity of the generated results. In this paper we develop a full-system system simulation environment for a mobile platform, which enables users to run a complete and unmodified software stack for a state-of-the-art mobile Arm CPU and Mali-G71 GPU powered device. We validate our simulator against a hardware implementation and Arm's stand-alone GPU simulator, achieving 100% architectural accuracy across all available toolchains. We demonstrate the capability of our GPU simulation framework by optimizing an advanced Computer Vision application using simulated statistics unavailable with other simulation approaches or physical GPU implementations. We demonstrate that performance optimizations for desktop GPUs trigger bottlenecks on mobile GPUs, and show the importance of efficient memory use.",
        "author": "Kuba Kaszyk and Harry Wagstaff and T. Spink and Bj\u00f6rn Franke and M. O\u2019Boyle and Bruno Bodin and Henrik Uhrenholt",
        "doi": "10.1109/ISPASS.2019.00015",
        "journal": "2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)",
        "keywords": "Computer simulation",
        "pages": "68-78",
        "title": "Full-System Simulation of Mobile CPU/GPU Platforms",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/8695656",
        "volume": "null",
        "year": "2019"
    },
    "Kim2020Profiling": {
        "abstract": "GPU profilers have been successfully used to analyze bottlenecks and slowdowns of GPU programs. Several instrumentation tools for profiling GPU binaries are introduced, but these tools take little consideration into GPU architectures. In this paper, we investigate common factors of performance degradation on existing GPU profilers and provide design guide to improve the performance.",
        "author": "Hwiwon Kim and Hyunjune Kim and Hwansoo Han",
        "doi": "10.1109/CASES51649.2020.9243727",
        "journal": "2020 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems (CASES)",
        "keywords": "GPU, CUDA, Profiler, Optimization",
        "pages": "17-19",
        "title": "Effective Profiling for Data-Intensive GPU Programs: Work-in-Progress",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9243727",
        "volume": "null",
        "year": "2020"
    },
    "Patel2021VirtualGPU": {
        "abstract": "While parallel programming is hard, programming accelerators has always been even more complicated. One fundamental reason is the lack of mature tooling that can be used to inspect a program that executes on two different architectures. As GPU software stacks of different vendors provide vastly different experience for developers, it is clear that the gold standard for debugging is still host (CPU) execution with its myriad of mature tooling options. In this work we present a virtual GPU (VGPU) OpenMP offloading target that allows to emulate a GPU execution environment on the host. In contrast to classical \u201chost offloading\u201d, the VGPU target reuses the same execution model, compilation paths, and runtimes as a physical GPU. While this execution mode is not able to perform as good as host-specific compilation, runtimes, and execution, it provides the developor with a more accurate stand-in for GPU offloading that is still amendable to existing host tooling.",
        "author": "A. Patel and Shilei Tian and J. Doerfert and B. Chapman",
        "doi": "10.1145/3458744.3473356",
        "journal": "50th International Conference on Parallel Processing Workshop",
        "keywords": "LLVM, OpenMP, accelerator offloading, GPU, debugging",
        "pages": "null",
        "title": "A Virtual GPU as Developer-Friendly OpenMP Offload Target",
        "type": "article",
        "url": "https://dl.acm.org/doi/10.1145/3458744.3473356",
        "volume": "null",
        "year": "2021"
    },
    "Perkins2016cltorch": {
        "abstract": "This paper presents cltorch, a hardware-agnostic backend for the Torch neural network framework. cltorch enables training of deep neural networks on GPUs from diverse hardware vendors, including AMD, NVIDIA, and Intel. cltorch contains sufficient implementation to run models such as AlexNet, VGG, Overfeat, and GoogleNet. It is written using the OpenCL language, a portable compute language, governed by the Khronos Group. cltorch is the top-ranked hardware-agnostic machine learning framework on Chintala's convnet-benchmarks page. This paper presents the technical challenges encountered whilst creating the cltorch backend for Torch, and looks in detail at the challenges related to obtaining a fast hardware-agnostic implementation. The convolutional layers are identified as the key area of focus for accelerating hardware-agnostic frameworks. Possible approaches to accelerating the convolutional implementation are identified including: implementation of the convolutions using the implicitgemm or winograd algorithm, using a GEMM implementation adapted to the geometries associated with the convolutional algorithm, or using a pluggable hardware-specific convolutional implementation.",
        "arxivid": "1606.04884",
        "author": "Hugh Perkins",
        "journal": "ArXiv",
        "keywords": "GPU, OpenCL, Parallel, Deep Neural Networks, MachineLearning, Convolution",
        "pages": "null",
        "title": "cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network Library, Based on OpenCL",
        "type": "article",
        "url": "https://www.semanticscholar.org/paper/5bdea483fa798d52240fde3ddcb750186466803a",
        "volume": "abs/1606.04884",
        "year": "2016"
    }
}});